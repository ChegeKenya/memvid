{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memvid on Colab with GPU Support\n",
    "\n",
    "This notebook allows you to run the Memvid project on Google Colab, leveraging a GPU for accelerated performance, particularly for embedding generation and semantic search.\n",
    "\n",
    "Follow the steps below to set up the environment and use Memvid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "This section checks GPU, clones the Memvid repository, and installs all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability (already run in the cell above, but can be re-checked)\n",
    "!nvidia-smi\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/featuregraph/memvid.git\n",
    "%cd memvid\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update && apt-get install -y ffmpeg libzbar0\n",
    "\n",
    "# Install PyTorch with CUDA\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install faiss-gpu and sentence-transformers\n",
    "!pip install faiss-gpu sentence-transformers\n",
    "\n",
    "# Install other Python dependencies (from requirements.txt, excluding conflicting/already installed ones)\n",
    "!pip install qrcode[pil] opencv-python opencv-contrib-python numpy tqdm Pillow PyPDF2 python-dotenv beautifulsoup4 ebooklib openai google-generativeai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure for GPU and Prepare Data\n",
    "\n",
    "This section configures Memvid to use the GPU for indexing and provides a space for your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import memvid.config\n",
    "from memvid import MemvidEncoder, quick_chat, chat_with_memory\n",
    "\n",
    "# --- Configure for GPU ---\n",
    "# Get the default config and modify it for this session\n",
    "print(\"Original default config for index:\", memvid.config.get_default_config()['index'])\n",
    "default_config = memvid.config.get_default_config()\n",
    "default_config['index']['use_gpu'] = True\n",
    "default_config['index']['type'] = 'Flat'  # 'Flat' index is robust for varying data sizes in examples\n",
    "print(\"Updated default config for index:\", default_config['index'])\n",
    "\n",
    "# --- Define Output Paths ---\n",
    "# Ensure you are in the 'memvid' directory cloned earlier\n",
    "# %cd /content/memvid \n",
    "# (The %cd magic should have been run in the setup cell. If not, uncomment the line above)\n",
    "output_dir = \"colab_output\" # Store outputs in a subdirectory within /content/memvid/\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_file_path = os.path.join(output_dir, \"memory_colab.mp4\") # Using .mp4 for broad compatibility\n",
    "index_file_path = os.path.join(output_dir, \"memory_colab_index\") # .json is added by save method\n",
    "\n",
    "print(f\"Video will be saved to: {os.path.abspath(video_file_path)}\")\n",
    "print(f\"Index will be saved to: {os.path.abspath(index_file_path)}.faiss and .json\")\n",
    "\n",
    "\n",
    "# --- Provide Your Text Data ---\n",
    "# Replace the example text below with your own data.\n",
    "user_text_data = \"\"\"\n",
    "The quantum computer achieved 100 qubits of processing power in March 2024.\n",
    "Machine learning models can now process over 1 trillion parameters efficiently.\n",
    "The new GPU architecture delivers 5x performance improvement for AI workloads.\n",
    "Cloud storage costs have decreased by 80% over the past five years.\n",
    "Quantum encryption methods are becoming standard for secure communications.\n",
    "Edge computing reduces latency to under 1ms for critical applications.\n",
    "Neural networks can now generate photorealistic images in real-time.\n",
    "\n",
    "This is a sample document for Memvid.\n",
    "Memvid allows storing text into video frames using QR codes.\n",
    "It uses sentence transformers for embeddings and FAISS for indexing.\n",
    "This notebook demonstrates running Memvid on Colab with GPU support.\n",
    "Make sure your FAISS index is configured to use the GPU for faster search.\n",
    "Sentence transformers will also benefit from GPU acceleration.\n",
    "\"\"\"\n",
    "print(f\"\\nProvided text data has {len(user_text_data)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Video Memory & Index\n",
    "\n",
    "This step encodes your text data into a video and creates a searchable index.\n",
    "If `use_gpu` was set to `True` in the config, FAISS indexing will utilize the GPU.\n",
    "SentenceTransformer embedding generation will also automatically use the GPU if PyTorch is set up with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Encoder ---\n",
    "# It will use the globally modified default_config if no specific config is passed.\n",
    "encoder = MemvidEncoder() \n",
    "print(f\"Encoder using config for index: {encoder.config['index']}\")\n",
    "\n",
    "\n",
    "# --- Add Text and Build Video ---\n",
    "print(\"\\nAdding text to encoder...\")\n",
    "encoder.add_text(user_text_data, chunk_size=256, overlap=50) # Adjusted chunk size for example\n",
    "\n",
    "stats = encoder.get_stats()\n",
    "print(f\"\\nEncoder stats before building:\")\n",
    "print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"  Total characters: {stats['total_characters']}\")\n",
    "\n",
    "print(f\"\\nBuilding video and index...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Note: VIDEO_FILE_TYPE from config is used by build_video to determine extension if not in output_file\n",
    "# We explicitly use .mp4 in video_file_path for this notebook.\n",
    "build_stats = encoder.build_video(video_file_path, index_file_path, show_progress=True, codec='mp4v') # Using mp4v for wider compatibility\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBuild completed in {elapsed:.2f} seconds.\")\n",
    "print(f\"Video file: {build_stats.get('video_file', 'Not found')}\")\n",
    "print(f\"Index file: {build_stats.get('index_file', 'Not found')}\")\n",
    "print(f\"Video duration: {build_stats.get('duration_seconds', 0):.1f}s\")\n",
    "print(f\"Video size: {build_stats.get('video_size_mb', 0):.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up LLM API Key (Optional)\n",
    "\n",
    "For chat functionality with an LLM (like OpenAI, Google, Anthropic), you need to provide an API key.\n",
    "It's recommended to use Colab's secret manager (click the key icon on the left sidebar) to store your API key.\n",
    "Then, you can access it like `os.environ.get('YOUR_SECRET_NAME')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata # For Colab secrets\n",
    "\n",
    "# Option 1: Use Colab Secrets (Recommended)\n",
    "# Create a secret named OPENAI_API_KEY (or GOOGLE_API_KEY, ANTHROPIC_API_KEY) in Colab's secrets manager\n",
    "# and put your API key there.\n",
    "try:\n",
    "    # Replace 'OPENAI_API_KEY' with the name of your secret if different\n",
    "    # Also change for other providers e.g. 'GOOGLE_API_KEY'\n",
    "    llm_api_key = userdata.get('OPENAI_API_KEY') \n",
    "    if llm_api_key:\n",
    "        print(\"Successfully loaded API key from Colab secrets.\")\n",
    "        # For OpenAI, it's often set as an environment variable\n",
    "        os.environ['OPENAI_API_KEY'] = llm_api_key \n",
    "        # For Google, the client usually takes it as a direct argument.\n",
    "        # For Anthropic, similar.\n",
    "    else:\n",
    "        print(\"API key not found in Colab secrets. Chat responses will be context-only or may fail.\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    print(\"Secret not found. Please create it in Colab's secret manager for LLM chat.\")\n",
    "    llm_api_key = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred accessing Colab secrets: {e}\")\n",
    "    llm_api_key = None\n",
    "\n",
    "# Option 2: Paste key directly (Less Secure - Use only for temporary testing)\n",
    "# if not llm_api_key:\n",
    "#   llm_api_key = \"sk-your-openai-api-key\" # Replace with your actual key\n",
    "#   os.environ['OPENAI_API_KEY'] = llm_api_key \n",
    "#   print(\"Used manually pasted API key.\")\n",
    "\n",
    "if not llm_api_key:\n",
    "    print(\"\\nLLM API key is not set. Chat functions might not provide full LLM responses or may only show retrieved context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Your Memory\n",
    "\n",
    "Now you can ask questions to your video memory.\n",
    "`quick_chat` sends a single query. `chat_with_memory` starts an interactive session.\n",
    "These functions use `MemvidChat` internally, which initializes `MemvidRetriever`.\n",
    "Since we updated the global default config, the retriever should also use the GPU for FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure paths are correct (they are defined in cell \"3. Configure for GPU and Prepare Data\")\n",
    "print(f\"Using video file: {video_file_path}\")\n",
    "print(f\"Using index file: {index_file_path}\") # IndexManager handles .faiss/.json extensions\n",
    "\n",
    "# --- Quick Chat Example ---\n",
    "if os.path.exists(video_file_path) and os.path.exists(index_file_path + \".faiss\"):\n",
    "    query = \"What is Memvid?\"\n",
    "    print(f\"\\nSending query to quick_chat: '{query}'\")\n",
    "    \n",
    "    # provider can be 'openai', 'google', 'anthropic'\n",
    "    # Ensure you have the corresponding API key set up and library installed\n",
    "    # For Google, you might need !pip install google-generativeai\n",
    "    # For Anthropic, you might need !pip install anthropic\n",
    "    # These should be in requirements.txt if intended for use.\n",
    "    # The llm_api_key is passed to the MemvidChat constructor if not found in env.\n",
    "    response = quick_chat(video_file_path, index_file_path, query, provider='openai', api_key=llm_api_key)\n",
    "    print(\"\\nResponse from quick_chat:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"\\nMemory files not found. Please run the 'Build Video Memory & Index' step successfully.\")\n",
    "\n",
    "# --- Interactive Chat Example (Optional) ---\n",
    "# Uncomment the lines below to start an interactive chat session.\n",
    "# print(\"\\nStarting interactive chat session (type 'quit' or 'exit' to end):\")\n",
    "# if os.path.exists(video_file_path) and os.path.exists(index_file_path + \".faiss\"):\n",
    "#    chat_with_memory(video_file_path, index_file_path, provider='openai', api_key=llm_api_key)\n",
    "# else:\n",
    "#    print(\"\\nMemory files not found for interactive chat.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
